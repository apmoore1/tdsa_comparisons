{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Baseline_non_target_results",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apmoore1/tdsa_comparisons/blob/master/analysis/Baseline_non_target_results.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJO1xoHLiQn2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!pip install -U git+git://github.com/apmoore1/target-extraction.git@master#egg=target-extraction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BQFGRCaiatz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict, Counter\n",
        "from tempfile import TemporaryDirectory\n",
        "from pathlib import Path\n",
        "from typing import List, Dict\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from target_extraction.data_types import TargetTextCollection\n",
        "from target_extraction.analysis.util import metric_df, long_format_metrics\n",
        "from target_extraction.analysis.util import add_metadata_to_df, combine_metrics\n",
        "from target_extraction.analysis.sentiment_metrics import accuracy, macro_f1\n",
        "from target_extraction.analysis.statistical_analysis import one_tailed_p_value\n",
        "\n",
        "def get_text_sentiment_distribution(dataset: TargetTextCollection\n",
        "                                    ) -> Dict[str, float]:\n",
        "    '''\n",
        "    :param dataset: Dataset.\n",
        "    :returns: A dictionary of the proportion of positive, negative, and neutral \n",
        "              samples in the given TargetTextCollection.\n",
        "    '''\n",
        "    text_sentiment_distribution = Counter()\n",
        "    data_size = len(dataset)\n",
        "    for target_text in dataset.values():\n",
        "        text_sentiment_distribution.update([target_text['text_sentiment']])\n",
        "    for key, value in text_sentiment_distribution.items():\n",
        "        text_sentiment_distribution[key] = round((value / data_size), 2) * 100\n",
        "    return dict(text_sentiment_distribution)\n",
        "\n",
        "def get_dataset_stats(collection: TargetTextCollection, version: str\n",
        "                      ) -> pd.DataFrame:\n",
        "  '''\n",
        "  :param collection: The TargetTextCollection to generate statistics for. \n",
        "  :param version: Either `single` or `average`.\n",
        "  :returns: A DataFrame containing the following columns and one value for each \n",
        "            column: `Size` - number of samples in the collection, `Version` - \n",
        "            the version argument, `Name` the collection's dataset name, \n",
        "            `positive` - the proportion of positive samples, `neutral` - the \n",
        "            proportion of neutral samples, and `negative` - the proportion of\n",
        "            negative samples.\n",
        "  '''\n",
        "  collection_stats = {**get_text_sentiment_distribution(collection)}\n",
        "  collection_stats['Size'] = len(collection)\n",
        "  collection_stats['Name'] = collection.name\n",
        "  collection_stats['Version'] = version\n",
        "  for key, value in collection_stats.items():\n",
        "    collection_stats[key] = [value] \n",
        "  return pd.DataFrame(collection_stats)\n",
        "\n",
        "\n",
        "def get_metric_results(collection: TargetTextCollection) -> pd.DataFrame:\n",
        "  '''\n",
        "  :param collection: Dataset that contains all of the results.\n",
        "  :returns: A pandas dataframe with the following columns: `['prediction key', \n",
        "            'run number', 'Accuracy', 'data-trained-on', 'Inter-Aspect', 'CWR', \n",
        "            'Position', 'Model', 'Macro F1', 'Dataset']`\n",
        "  '''\n",
        "  predicted_key = list(collection.metadata['predicted_target_sentiment_key'].keys())\n",
        "  acc_df = metric_df(collection, accuracy, 'target_sentiments', predicted_key,\n",
        "                     array_scores=True, assert_number_labels=3, \n",
        "                     metric_name='Accuracy', average=False, include_run_number=True)\n",
        "  acc_df = add_metadata_to_df(acc_df, collection, 'predicted_target_sentiment_key')\n",
        "  f1_df = metric_df(collection, macro_f1, 'target_sentiments', predicted_key,\n",
        "                    array_scores=True, assert_number_labels=3, \n",
        "                    metric_name='Macro F1', average=False, include_run_number=True)\n",
        "  combined_df = combine_metrics(acc_df, f1_df, 'Macro F1')\n",
        "  combined_df['Dataset'] = [collection.name] * combined_df.shape[0]\n",
        "  combined_df['Data Split'] = [collection.metadata['split']] * combined_df.shape[0]\n",
        "  return combined_df\n",
        "\n",
        "def mean_std(data: pd.Series) -> str:\n",
        "   to_percentage = data * 100\n",
        "   return f'{np.mean(to_percentage):.2f} ({np.std(to_percentage):.2f})'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r07DaNgT-Fep",
        "colab_type": "text"
      },
      "source": [
        "# Baseline non-target results\n",
        "In this notebook we are comparing the *CNN(single)* and *CNN(avg)* to see which version performs better. To do see we first need to download the data from the [relevant directory within the github repository](https://github.com/apmoore1/tdsa_comparisons/tree/master/saved_results/non_target_baselines) of which the code to do this is below. Furthermore the code when loading the data also calculates the relevant metric scores (accuracy and macro f1), finds all the relevant metadata, and creates the dataset statistics for the different training datasets. All of this data is then loaded into two pandas dataframe, one for the metrics on the validation and test splits and another dataframe for the dataset statistics for single and avergae training splits:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VSVmst8i4vn",
        "colab_type": "code",
        "outputId": "5ebab1fe-7f73-41e4-ecdf-dc06bef04ba9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Get the data\n",
        "\n",
        "result_base_url = Path('raw.githubusercontent.com/apmoore1/tdsa_'\n",
        "                       'comparisons/master/saved_results/non_target_baselines/')\n",
        "cnn_versions = ['single', 'average']\n",
        "data_splits = ['test', 'val']\n",
        "dataset_names = ['election', 'laptop', 'restaurant']\n",
        "\n",
        "all_results: List[pd.DataFrame] = []\n",
        "\n",
        "for cnn_version in cnn_versions:\n",
        "  for data_split in data_splits:\n",
        "    for dataset_name in dataset_names:\n",
        "      data_url = Path(result_base_url, cnn_version, f'{dataset_name}_dataset', \n",
        "                      f'{data_split}.json')\n",
        "      data_url = f'https://{str(data_url)}'\n",
        "      with TemporaryDirectory() as temp_dir:\n",
        "        temp_file = Path(temp_dir, 'temp_file')\n",
        "        response = requests.get(data_url, stream=True)\n",
        "        with temp_file.open('wb+') as fp:\n",
        "          for chunk in response.iter_content(chunk_size=128):\n",
        "            fp.write(chunk)\n",
        "        data_collection = TargetTextCollection.load_json(temp_file)\n",
        "        all_results.append(get_metric_results(data_collection))\n",
        "\n",
        "results_df = pd.concat(all_results, ignore_index=True)\n",
        "# Remove all of the results that contain the CWR\n",
        "test_result_df = results_df[results_df['Data Split']=='Test']\n",
        "test_result_df = test_result_df[test_result_df['CWR']==False]\n",
        "val_result_df = results_df[results_df['Data Split']=='Validation']\n",
        "val_result_df = val_result_df[val_result_df['CWR']==False]\n",
        "\n",
        "training_dataset_stats: List[pd.DataFrame] = []\n",
        "for cnn_version in cnn_versions:\n",
        "  for dataset_name in dataset_names:\n",
        "    data_url = Path(result_base_url, cnn_version, f'{dataset_name}_dataset', \n",
        "                    'train.json')\n",
        "    data_url = f'https://{str(data_url)}'\n",
        "    with TemporaryDirectory() as temp_dir:\n",
        "      temp_file = Path(temp_dir, 'temp_file')\n",
        "      response = requests.get(data_url, stream=True)\n",
        "      with temp_file.open('wb+') as fp:\n",
        "        for chunk in response.iter_content(chunk_size=128):\n",
        "          fp.write(chunk)\n",
        "      data_collection = TargetTextCollection.load_json(temp_file)\n",
        "      data_collection.name = dataset_name.capitalize()\n",
        "      training_dataset_stats.append(get_dataset_stats(data_collection, \n",
        "                                    version=cnn_version))\n",
        "training_dataset_stats = pd.concat(training_dataset_stats, ignore_index=True, \n",
        "                                   sort=False)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rb_EvsQSSKcI",
        "colab_type": "text"
      },
      "source": [
        "As stated above the data is loaded into a pandas DataFrame. For convience we split the entire dataframe into two, one for the test split results `test_result_df` and the other for the validation results `val_result_df`. Below shows the top 5 rows from the validation results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ElvJLUaSjrS",
        "colab_type": "code",
        "outputId": "194ebaa0-84fd-41bf-9ae8-40fe41c6202c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "val_result_df.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>prediction key</th>\n",
              "      <th>run number</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Inter-Aspect</th>\n",
              "      <th>CWR</th>\n",
              "      <th>Position</th>\n",
              "      <th>Model</th>\n",
              "      <th>data-trained-on</th>\n",
              "      <th>Macro F1</th>\n",
              "      <th>Dataset</th>\n",
              "      <th>Data Split</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>predicted_target_sentiment_single_GloVe</td>\n",
              "      <td>0</td>\n",
              "      <td>0.540636</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>CNN</td>\n",
              "      <td>single</td>\n",
              "      <td>0.422622</td>\n",
              "      <td>Election</td>\n",
              "      <td>Validation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>predicted_target_sentiment_single_GloVe</td>\n",
              "      <td>1</td>\n",
              "      <td>0.548488</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>CNN</td>\n",
              "      <td>single</td>\n",
              "      <td>0.392064</td>\n",
              "      <td>Election</td>\n",
              "      <td>Validation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>predicted_target_sentiment_single_GloVe</td>\n",
              "      <td>2</td>\n",
              "      <td>0.542599</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>CNN</td>\n",
              "      <td>single</td>\n",
              "      <td>0.409426</td>\n",
              "      <td>Election</td>\n",
              "      <td>Validation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>predicted_target_sentiment_single_GloVe</td>\n",
              "      <td>3</td>\n",
              "      <td>0.547703</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>CNN</td>\n",
              "      <td>single</td>\n",
              "      <td>0.370552</td>\n",
              "      <td>Election</td>\n",
              "      <td>Validation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>predicted_target_sentiment_single_GloVe</td>\n",
              "      <td>4</td>\n",
              "      <td>0.549274</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>CNN</td>\n",
              "      <td>single</td>\n",
              "      <td>0.398766</td>\n",
              "      <td>Election</td>\n",
              "      <td>Validation</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                             prediction key  run number  ...   Dataset  Data Split\n",
              "24  predicted_target_sentiment_single_GloVe           0  ...  Election  Validation\n",
              "25  predicted_target_sentiment_single_GloVe           1  ...  Election  Validation\n",
              "26  predicted_target_sentiment_single_GloVe           2  ...  Election  Validation\n",
              "27  predicted_target_sentiment_single_GloVe           3  ...  Election  Validation\n",
              "28  predicted_target_sentiment_single_GloVe           4  ...  Election  Validation\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTNfnLb3veIV",
        "colab_type": "text"
      },
      "source": [
        "Below shows the training dataset statistics for the two CNN versions. From this we can see that the proportion of samples within the sentiment classes are very similar between the two. However the major difference is the absolute sample size where average is always larger as expected but in the Election case almost 100% larger. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRJu4PdTvmSX",
        "colab_type": "code",
        "outputId": "0795f9c0-5185-4b95-e32c-0bf68d752fd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        }
      },
      "source": [
        "pd.pivot_table(training_dataset_stats, index='Name', \n",
        "               values=['negative', 'neutral', 'positive', 'Size'], \n",
        "               columns='Version').T"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>Election</th>\n",
              "      <th>Laptop</th>\n",
              "      <th>Restaurant</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>Version</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">Size</th>\n",
              "      <th>average</th>\n",
              "      <td>2319.0</td>\n",
              "      <td>1051.0</td>\n",
              "      <td>1378.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>single</th>\n",
              "      <td>1227.0</td>\n",
              "      <td>933.0</td>\n",
              "      <td>1162.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">negative</th>\n",
              "      <th>average</th>\n",
              "      <td>49.0</td>\n",
              "      <td>43.0</td>\n",
              "      <td>24.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>single</th>\n",
              "      <td>52.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>21.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">neutral</th>\n",
              "      <th>average</th>\n",
              "      <td>37.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>16.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>single</th>\n",
              "      <td>37.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>14.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">positive</th>\n",
              "      <th>average</th>\n",
              "      <td>15.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>60.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>single</th>\n",
              "      <td>11.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>64.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Name              Election  Laptop  Restaurant\n",
              "         Version                              \n",
              "Size     average    2319.0  1051.0      1378.0\n",
              "         single     1227.0   933.0      1162.0\n",
              "negative average      49.0    43.0        24.0\n",
              "         single       52.0    44.0        21.0\n",
              "neutral  average      37.0    17.0        16.0\n",
              "         single       37.0    14.0        14.0\n",
              "positive average      15.0    40.0        60.0\n",
              "         single       11.0    42.0        64.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Nfjtk5GSnGw",
        "colab_type": "text"
      },
      "source": [
        "As we can see the dataframe contains all the relevant data and some columns that are not needed. The list here explains all of the columns that are relevant:\n",
        "1. `data-trained-on` -- this states which version of the CNN it is. This will be either `single` which means that the CNN model was only trained on sentences that contain one unique sentiment. Or `average` where the model was trained on all sentences and the sentiment for each sentiment was the most frequent target sentiment in that sentence.\n",
        "2. `run number` -- This determines how many times a model was trained and tested for to take into account of the random seed problem. Each model was trained and tested 8 times thus `run number` ranges from 0-7.\n",
        "3. `Dataset` -- the dataset that the model was trained and tested on. This can only be `Election`, `Restaurant`, or `Laptop`.\n",
        "4. `Data Split` -- which part of the dataset the results are associated with. This can only be `Test` or `Validation`. \n",
        "5. `prediction key` -- This is a unique identifier per model where one trained model can be uniquely identified by combining the `prediction key` and the `run number` columns.\n",
        "\n",
        "Now that we know the data better we can generate the results for both the *Test* and *Validation* data splits. The results generated will be for both *Accuracy* and *Macro F1* metrics, and across all 3 datasets. The results will compare the *CNN (single)* to *CNN (average)*. Lastly as we have run each version of the model **8** times to take into account the random seed problem the results will show the mean score and the standard deviation in brackets.\n",
        "\n",
        "Validation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tocnvZa-wUXM",
        "colab_type": "code",
        "outputId": "498ba08a-1c6c-4b3a-d80f-147f0e3f54c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        }
      },
      "source": [
        "val_metric_results = pd.pivot_table(val_result_df, index='data-trained-on', \n",
        "                                    values=['Accuracy', 'Macro F1'], \n",
        "                                    columns='Dataset', \n",
        "                                    aggfunc={'Accuracy': mean_std, \n",
        "                                             'Macro F1': mean_std})\n",
        "val_metric_results.T"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>data-trained-on</th>\n",
              "      <th>average</th>\n",
              "      <th>single</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>Dataset</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"3\" valign=\"top\">Accuracy</th>\n",
              "      <th>Election</th>\n",
              "      <td>54.07 (0.56)</td>\n",
              "      <td>54.54 (0.43)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Laptop</th>\n",
              "      <td>70.65 (0.68)</td>\n",
              "      <td>69.46 (0.72)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Restaurant</th>\n",
              "      <td>72.31 (0.69)</td>\n",
              "      <td>71.98 (0.41)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"3\" valign=\"top\">Macro F1</th>\n",
              "      <th>Election</th>\n",
              "      <td>42.74 (2.09)</td>\n",
              "      <td>39.62 (1.75)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Laptop</th>\n",
              "      <td>66.32 (0.96)</td>\n",
              "      <td>63.33 (1.70)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Restaurant</th>\n",
              "      <td>60.51 (1.20)</td>\n",
              "      <td>58.74 (1.44)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "data-trained-on           average        single\n",
              "         Dataset                               \n",
              "Accuracy Election    54.07 (0.56)  54.54 (0.43)\n",
              "         Laptop      70.65 (0.68)  69.46 (0.72)\n",
              "         Restaurant  72.31 (0.69)  71.98 (0.41)\n",
              "Macro F1 Election    42.74 (2.09)  39.62 (1.75)\n",
              "         Laptop      66.32 (0.96)  63.33 (1.70)\n",
              "         Restaurant  60.51 (1.20)  58.74 (1.44)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsgn9VZ1XB1O",
        "colab_type": "text"
      },
      "source": [
        "Test:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjkvcC2owpgM",
        "colab_type": "code",
        "outputId": "9e471954-8c02-414d-d57b-3c3a27e67e0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        }
      },
      "source": [
        "test_metric_results = pd.pivot_table(test_result_df, index='data-trained-on', \n",
        "                                     values=['Accuracy', 'Macro F1'], \n",
        "                                     columns='Dataset', \n",
        "                                     aggfunc={'Accuracy': mean_std, \n",
        "                                              'Macro F1': mean_std})\n",
        "test_metric_results.T"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>data-trained-on</th>\n",
              "      <th>average</th>\n",
              "      <th>single</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>Dataset</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"3\" valign=\"top\">Accuracy</th>\n",
              "      <th>Election</th>\n",
              "      <td>52.35 (0.69)</td>\n",
              "      <td>54.29 (0.73)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Laptop</th>\n",
              "      <td>68.26 (0.69)</td>\n",
              "      <td>65.99 (0.80)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Restaurant</th>\n",
              "      <td>75.81 (0.55)</td>\n",
              "      <td>75.19 (0.94)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"3\" valign=\"top\">Macro F1</th>\n",
              "      <th>Election</th>\n",
              "      <td>39.98 (2.20)</td>\n",
              "      <td>39.73 (1.88)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Laptop</th>\n",
              "      <td>60.43 (1.36)</td>\n",
              "      <td>55.36 (2.00)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Restaurant</th>\n",
              "      <td>59.40 (1.52)</td>\n",
              "      <td>56.71 (1.63)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "data-trained-on           average        single\n",
              "         Dataset                               \n",
              "Accuracy Election    52.35 (0.69)  54.29 (0.73)\n",
              "         Laptop      68.26 (0.69)  65.99 (0.80)\n",
              "         Restaurant  75.81 (0.55)  75.19 (0.94)\n",
              "Macro F1 Election    39.98 (2.20)  39.73 (1.88)\n",
              "         Laptop      60.43 (1.36)  55.36 (2.00)\n",
              "         Restaurant  59.40 (1.52)  56.71 (1.63)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qznzo7twXZra",
        "colab_type": "text"
      },
      "source": [
        "We can see from the results are consistent across the data splits (Test and Validation) as both contain the same ordering where *CNN (avg)* is better than *CNN (single)* for all but the Accuracy on the Election dataset. This therefore shows that in general it is better to use more of the data even if the sentiment label is noisy. \n",
        "\n",
        "However the difference between *CNN (single)* and *CNN (average)* can be quite small, which is shown better below where the table shows the difference between *CNN (avg)* and *CNN (single)*.\n",
        "\n",
        "Validation score differences:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOL4Zw3oF3N6",
        "colab_type": "code",
        "outputId": "efbb187b-cae9-4338-8c4a-0e88ebf9ef04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "def metric_differences(data_split_df: pd.DataFrame, metric_names: List[str]) -> pd.DataFrame:\n",
        "  '''\n",
        "  Returns the difference between the Average and Single version\n",
        "\n",
        "  :param data_split_df: Dataframe the contains the following columns \n",
        "                        `data-trained-on` and all the strings within the \n",
        "                        `metric_names` argument.\n",
        "  :param metric_names: Names of columns that contain metric scores that are \n",
        "                       to be compared.\n",
        "  :returns: A dataframe that contains new `Difference` columns.\n",
        "  '''\n",
        "  temp_df = data_split_df.copy(deep=True)\n",
        "  temp_df = temp_df.set_index(['Dataset', 'run number'])\n",
        "  average_df = temp_df[temp_df['data-trained-on']=='average']\n",
        "  single_df = temp_df[temp_df['data-trained-on']=='single']\n",
        "  for metric_name in metric_names:\n",
        "    diff_df = average_df[metric_name] - single_df[metric_name]\n",
        "    temp_df[f'Difference {metric_name}'] = diff_df\n",
        "  temp_df = temp_df.reset_index()\n",
        "  temp_df = temp_df[temp_df['data-trained-on']=='average']\n",
        "  return temp_df\n",
        "\n",
        "validation_diff_df = metric_differences(val_result_df, ['Accuracy', 'Macro F1'])\n",
        "pd.pivot_table(validation_diff_df, index='Dataset',\n",
        "               values=['Difference Accuracy', 'Difference Macro F1'], \n",
        "               aggfunc={'Difference Accuracy': mean_std, \n",
        "                        'Difference Macro F1': mean_std})"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Difference Accuracy</th>\n",
              "      <th>Difference Macro F1</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dataset</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Election</th>\n",
              "      <td>-0.48 (0.67)</td>\n",
              "      <td>3.12 (2.58)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Laptop</th>\n",
              "      <td>1.19 (1.14)</td>\n",
              "      <td>2.99 (2.18)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Restaurant</th>\n",
              "      <td>0.34 (0.74)</td>\n",
              "      <td>1.77 (1.69)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           Difference Accuracy Difference Macro F1\n",
              "Dataset                                           \n",
              "Election          -0.48 (0.67)         3.12 (2.58)\n",
              "Laptop             1.19 (1.14)         2.99 (2.18)\n",
              "Restaurant         0.34 (0.74)         1.77 (1.69)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkefO8H5MPT4",
        "colab_type": "text"
      },
      "source": [
        "Test:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I99RWdajY1qY",
        "colab_type": "code",
        "outputId": "1afd736d-968d-4b59-d60c-3ff908603874",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "test_diff_df = metric_differences(test_result_df, ['Accuracy', 'Macro F1'])\n",
        "pd.pivot_table(test_diff_df, index='Dataset',\n",
        "               values=['Difference Accuracy', 'Difference Macro F1'], \n",
        "               aggfunc={'Difference Accuracy': mean_std, \n",
        "                        'Difference Macro F1': mean_std})"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Difference Accuracy</th>\n",
              "      <th>Difference Macro F1</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dataset</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Election</th>\n",
              "      <td>-1.94 (1.36)</td>\n",
              "      <td>0.24 (3.00)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Laptop</th>\n",
              "      <td>2.27 (1.00)</td>\n",
              "      <td>5.07 (2.76)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Restaurant</th>\n",
              "      <td>0.62 (1.31)</td>\n",
              "      <td>2.69 (2.82)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           Difference Accuracy Difference Macro F1\n",
              "Dataset                                           \n",
              "Election          -1.94 (1.36)         0.24 (3.00)\n",
              "Laptop             2.27 (1.00)         5.07 (2.76)\n",
              "Restaurant         0.62 (1.31)         2.69 (2.82)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Di31XcF5M7m9",
        "colab_type": "text"
      },
      "source": [
        "As we can see the differences between *CNN (avg)* and *CNN(single)* is very small espically when you take into account the standard deviation. The dataset that the *CNN (avg)* espically performs better is the Laptop dataset. This is most likely due to the fact that the Laptop dataset contains a lot more sentences that only have one unique sentiment (*DS1*).\n",
        "\n",
        "Below we show if the *CNN (avg)* is statistically significantly better then *CNN (single)* using a one-tailed t-test for each of the metrics. For the accuracy metric we use the Welch's t-test as we can assume normality but the macro f1 we cannot therefore we use the Wilcoxon signed-rank test [Dror and Reichart 2018](https://arxiv.org/pdf/1809.01448.pdf). Furthermore we want to know if *CNN (avg)* is statistically significantly better then *CNN (single)* only on the datasets that *CNN (avg)* is better which is all the datasets for the macro-f1 metric and all but the Election for the accuracy metric.\n",
        "\n",
        "Validation and Test:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Us41V7H05kxx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def metric_p_values(data_split_df: pd.DataFrame, better_split: str, \n",
        "                    compare_split: str, datasets: List[str], \n",
        "                    metric_names: List[str], assume_metric_normal: List[bool]) -> pd.DataFrame:\n",
        "  '''\n",
        "  Returns the difference between the Average and Single version\n",
        "\n",
        "  :param data_split_df: Dataframe the contains the following columns \n",
        "                        `data-trained-on` and all the strings within the \n",
        "                        `metric_names` argument.\n",
        "  :param metric_names: Names of columns that contain metric scores that are \n",
        "                       to be compared.\n",
        "  :returns: A dataframe that contains new `Difference` columns.\n",
        "  '''\n",
        "  temp_df = data_split_df.copy(deep=True)\n",
        "  better_df = temp_df[temp_df['data-trained-on']==f'{better_split}']\n",
        "  compare_df = temp_df[temp_df['data-trained-on']==f'{compare_split}']\n",
        "  metric_dataset_p_value = defaultdict(dict)\n",
        "  for dataset in datasets:\n",
        "    better_dataset_df = better_df[better_df['Dataset']==dataset]\n",
        "    compare_dataset_df = compare_df[compare_df['Dataset']==dataset]\n",
        "    for metric_index, metric_name in enumerate(metric_names):\n",
        "      assume_normal = assume_metric_normal[metric_index]\n",
        "      better_scores = better_dataset_df[f'{metric_name}']\n",
        "      compare_scores = compare_dataset_df[f'{metric_name}']\n",
        "      p_value = one_tailed_p_value(better_scores, compare_scores, \n",
        "                                   assume_normal=assume_normal)\n",
        "      metric_dataset_p_value[dataset][metric_name] = p_value\n",
        "  return metric_dataset_p_value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-v24okXwwfK9",
        "colab_type": "code",
        "outputId": "98b79991-6a0a-4525-d01e-502aaa7a712c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "metric_names = ['Accuracy', 'Macro F1']\n",
        "all_dataset_names = ['Election', 'Laptop', 'Restaurant']\n",
        "\n",
        "validation_p_values = pd.DataFrame(metric_p_values(val_result_df, 'average', \n",
        "                                   'single', all_dataset_names, metric_names, \n",
        "                                   [True, False]))\n",
        "test_p_values = pd.DataFrame(metric_p_values(test_result_df, 'average', \n",
        "                             'single', all_dataset_names, metric_names, \n",
        "                             [True, False]))\n",
        "combined_test_p_values = test_p_values.reset_index()\n",
        "combined_test_p_values['Split'] = ['Test'] * combined_test_p_values.shape[0]\n",
        "combined_validation_p_values = validation_p_values.reset_index()\n",
        "combined_validation_p_values['Split'] = ['Validation'] * combined_validation_p_values.shape[0]\n",
        "combined_p_values = pd.concat([combined_test_p_values, combined_validation_p_values], \n",
        "                              ignore_index=True)\n",
        "combined_p_values = combined_p_values.rename(columns={\"index\": \"Metric\", \"B\": \"c\"})\n",
        "combined_p_values = pd.melt(combined_p_values, id_vars=['Metric', 'Split'], \n",
        "                            value_vars=['Election', 'Laptop', 'Restaurant'], \n",
        "                            var_name='Dataset', value_name='P-Value')\n",
        "pd.pivot_table(combined_p_values, index=['Split', 'Metric'], \n",
        "               columns='Dataset', values='P-Value')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/stats/morestats.py:2863: UserWarning: Sample size too small for normal approximation.\n",
            "  warnings.warn(\"Sample size too small for normal approximation.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Dataset</th>\n",
              "      <th>Election</th>\n",
              "      <th>Laptop</th>\n",
              "      <th>Restaurant</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Split</th>\n",
              "      <th>Metric</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">Test</th>\n",
              "      <th>Accuracy</th>\n",
              "      <td>0.999921</td>\n",
              "      <td>0.000029</td>\n",
              "      <td>0.077711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Macro F1</th>\n",
              "      <td>0.444319</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.024975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">Validation</th>\n",
              "      <th>Accuracy</th>\n",
              "      <td>0.950955</td>\n",
              "      <td>0.003293</td>\n",
              "      <td>0.144942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Macro F1</th>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.017846</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Dataset              Election    Laptop  Restaurant\n",
              "Split      Metric                                  \n",
              "Test       Accuracy  0.999921  0.000029    0.077711\n",
              "           Macro F1  0.444319  0.005859    0.024975\n",
              "Validation Accuracy  0.950955  0.003293    0.144942\n",
              "           Macro F1  0.005859  0.005859    0.017846"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtRKuC3iSkgJ",
        "colab_type": "text"
      },
      "source": [
        "We can see above that *CNN (avg)* is only statistically significantly better than *CNN (single)* on the Laptop dataset for both metrics and the Restaurant datsset for for the Macro F1 metric with an $\\alpha < 0.05$. Furthermore if we use the Bonferroni correction to take into account comparing multiple P-Values for each of the metrics we find the following: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-DFSIPwTqyi",
        "colab_type": "code",
        "outputId": "348165f7-fb15-4624-cdef-2ac3c133d578",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "from target_extraction.analysis.statistical_analysis import find_k_estimator\n",
        "\n",
        "print('Validation Split')\n",
        "for metric_name in metric_names:\n",
        "  sig_datasets = find_k_estimator(validation_p_values.loc[f'{metric_name}', :].values, \n",
        "                                  alpha=0.05, method='B')\n",
        "  print(f'For metric {metric_name} there are {sig_datasets} where CNN (avg) is '\n",
        "        'statistically significantly better than CNN (single) with a '\n",
        "        'confindence >=95%')\n",
        "  \n",
        "\n",
        "print('Test Split')\n",
        "for metric_name in metric_names:\n",
        "  sig_datasets = find_k_estimator(test_p_values.loc[f'{metric_name}', :].values, \n",
        "                                  alpha=0.05, method='B')\n",
        "  print(f'For metric {metric_name} there are {sig_datasets} where CNN (avg) is '\n",
        "        'statistically significantly better than CNN (single) with a '\n",
        "        'confindence >=95%')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation Split\n",
            "For metric Accuracy there are 1 where CNN (avg) is statistically significantly better than CNN (single) with a confindence >=95%\n",
            "For metric Macro F1 there are 3 where CNN (avg) is statistically significantly better than CNN (single) with a confindence >=95%\n",
            "Test Split\n",
            "For metric Accuracy there are 1 where CNN (avg) is statistically significantly better than CNN (single) with a confindence >=95%\n",
            "For metric Macro F1 there are 2 where CNN (avg) is statistically significantly better than CNN (single) with a confindence >=95%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osENbHXhVieZ",
        "colab_type": "text"
      },
      "source": [
        "To conclude it is better to use the *CNN (avg)* as a strong baseline across the datasets. Furthermore *CNN (avg)* performance is far superior when using the Macro F1 metric, of which this is most likely due to the very few samples the *single* dataset has for the minority class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkjPJ7dqVShh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}