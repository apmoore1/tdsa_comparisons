{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Baseline_non_target_results",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apmoore1/tdsa_comparisons/blob/master/analysis/Baseline_non_target_results.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJO1xoHLiQn2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!pip install -U git+git://github.com/apmoore1/target-extraction.git@master#egg=target-extraction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BQFGRCaiatz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "from tempfile import TemporaryDirectory\n",
        "from pathlib import Path\n",
        "from typing import List\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from target_extraction.data_types import TargetTextCollection\n",
        "from target_extraction.analysis.util import metric_df, long_format_metrics\n",
        "from target_extraction.analysis.util import add_metadata_to_df, combine_metrics\n",
        "from target_extraction.analysis.sentiment_metrics import accuracy, macro_f1\n",
        "from target_extraction.analysis.statistical_analysis import one_tailed_p_value\n",
        "\n",
        "def get_metric_results(collection: TargetTextCollection) -> pd.DataFrame:\n",
        "  '''\n",
        "  :param collection: Dataset that contains all of the results.\n",
        "  :returns: A pandas dataframe with the following columns: `['prediction key', \n",
        "            'run number', 'Accuracy', 'data-trained-on', 'Inter-Aspect', 'CWR', \n",
        "            'Position', 'Model', 'Macro F1', 'Dataset']`\n",
        "  '''\n",
        "  predicted_key = list(collection.metadata['predicted_target_sentiment_key'].keys())\n",
        "  acc_df = metric_df(collection, accuracy, 'target_sentiments', predicted_key,\n",
        "                     array_scores=True, assert_number_labels=3, \n",
        "                     metric_name='Accuracy', average=False, include_run_number=True)\n",
        "  acc_df = add_metadata_to_df(acc_df, collection, 'predicted_target_sentiment_key')\n",
        "  f1_df = metric_df(collection, macro_f1, 'target_sentiments', predicted_key,\n",
        "                    array_scores=True, assert_number_labels=3, \n",
        "                    metric_name='Macro F1', average=False, include_run_number=True)\n",
        "  combined_df = combine_metrics(acc_df, f1_df, 'Macro F1')\n",
        "  combined_df['Dataset'] = [collection.name] * combined_df.shape[0]\n",
        "  combined_df['Data Split'] = [collection.metadata['split']] * combined_df.shape[0]\n",
        "  return combined_df\n",
        "\n",
        "def mean_std(data: pd.Series) -> str:\n",
        "   to_percentage = data * 100\n",
        "   return f'{np.mean(to_percentage):.2f} ({np.std(to_percentage):.2f})'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r07DaNgT-Fep",
        "colab_type": "text"
      },
      "source": [
        "# Baseline non-target results\n",
        "In this notebook we are comparing the *CNN(single)* and *CNN(avg)* to see which version performs better. To do see we first need to download the data from the [relevant directory within the github repository](https://github.com/apmoore1/tdsa_comparisons/tree/master/saved_results/non_target_baselines) of which the code to do this is below. Furthermore the code when loading the data also calculates the relevant metric scores (accuracy and macro f1) and finds all the relevant metadata. All of this data is then loaded into a pandas dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VSVmst8i4vn",
        "colab_type": "code",
        "outputId": "32f60694-2647-4af0-a87d-c134d2bf0939",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Get the data\n",
        "\n",
        "result_base_url = Path('raw.githubusercontent.com/apmoore1/tdsa_'\n",
        "                       'comparisons/master/saved_results/non_target_baselines/')\n",
        "cnn_versions = ['single', 'average']\n",
        "data_splits = ['test', 'val']\n",
        "dataset_names = ['election', 'laptop', 'restaurant']\n",
        "\n",
        "all_results: List[pd.DataFrame] = []\n",
        "\n",
        "for cnn_version in cnn_versions:\n",
        "  for data_split in data_splits:\n",
        "    for dataset_name in dataset_names:\n",
        "      data_url = Path(result_base_url, cnn_version, f'{dataset_name}_dataset', \n",
        "                      f'{data_split}.json')\n",
        "      data_url = f'https://{str(data_url)}'\n",
        "      with TemporaryDirectory() as temp_dir:\n",
        "        temp_file = Path(temp_dir, 'temp_file')\n",
        "        response = requests.get(data_url, stream=True)\n",
        "        with temp_file.open('wb+') as fp:\n",
        "          for chunk in response.iter_content(chunk_size=128):\n",
        "            fp.write(chunk)\n",
        "        data_collection = TargetTextCollection.load_json(temp_file)\n",
        "        all_results.append(get_metric_results(data_collection))\n",
        "\n",
        "results_df = pd.concat(all_results, ignore_index=True)\n",
        "test_result_df = results_df[results_df['Data Split']=='Test']\n",
        "val_result_df = results_df[results_df['Data Split']=='Validation']"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rb_EvsQSSKcI",
        "colab_type": "text"
      },
      "source": [
        "As stated above the data is loaded into a pandas DataFrame. For convience we split the entire dataframe into two, one for the test split results `test_result_df` and the other for the validation results `val_result_df`. Below shows the top 5 rows from the validation results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ElvJLUaSjrS",
        "colab_type": "code",
        "outputId": "91914b2a-83f5-4aa0-dcef-036907966866",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "val_result_df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>prediction key</th>\n",
              "      <th>run number</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Inter-Aspect</th>\n",
              "      <th>Position</th>\n",
              "      <th>Model</th>\n",
              "      <th>CWR</th>\n",
              "      <th>data-trained-on</th>\n",
              "      <th>Macro F1</th>\n",
              "      <th>Dataset</th>\n",
              "      <th>Data Split</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>predicted_target_sentiment_single_GloVe</td>\n",
              "      <td>0</td>\n",
              "      <td>0.540636</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>CNN</td>\n",
              "      <td>False</td>\n",
              "      <td>single</td>\n",
              "      <td>0.422622</td>\n",
              "      <td>Election</td>\n",
              "      <td>Validation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>predicted_target_sentiment_single_GloVe</td>\n",
              "      <td>1</td>\n",
              "      <td>0.548488</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>CNN</td>\n",
              "      <td>False</td>\n",
              "      <td>single</td>\n",
              "      <td>0.392064</td>\n",
              "      <td>Election</td>\n",
              "      <td>Validation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>predicted_target_sentiment_single_GloVe</td>\n",
              "      <td>2</td>\n",
              "      <td>0.542599</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>CNN</td>\n",
              "      <td>False</td>\n",
              "      <td>single</td>\n",
              "      <td>0.409426</td>\n",
              "      <td>Election</td>\n",
              "      <td>Validation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>predicted_target_sentiment_single_GloVe</td>\n",
              "      <td>3</td>\n",
              "      <td>0.547703</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>CNN</td>\n",
              "      <td>False</td>\n",
              "      <td>single</td>\n",
              "      <td>0.370552</td>\n",
              "      <td>Election</td>\n",
              "      <td>Validation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>predicted_target_sentiment_single_GloVe</td>\n",
              "      <td>4</td>\n",
              "      <td>0.549274</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>CNN</td>\n",
              "      <td>False</td>\n",
              "      <td>single</td>\n",
              "      <td>0.398766</td>\n",
              "      <td>Election</td>\n",
              "      <td>Validation</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                             prediction key  run number  ...   Dataset  Data Split\n",
              "24  predicted_target_sentiment_single_GloVe           0  ...  Election  Validation\n",
              "25  predicted_target_sentiment_single_GloVe           1  ...  Election  Validation\n",
              "26  predicted_target_sentiment_single_GloVe           2  ...  Election  Validation\n",
              "27  predicted_target_sentiment_single_GloVe           3  ...  Election  Validation\n",
              "28  predicted_target_sentiment_single_GloVe           4  ...  Election  Validation\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Nfjtk5GSnGw",
        "colab_type": "text"
      },
      "source": [
        "As we can see the dataframe contains all the relevant data and some columns that are not needed. The list here explains all of the columns that are relevant:\n",
        "1. `data-trained-on` -- this states which version of the CNN it is. This will be either `single` which means that the CNN model was only trained on sentences that contain one unique sentiment. Or `average` where the model was trained on all sentences and the sentiment for each sentiment was the most frequent target sentiment in that sentence.\n",
        "2. `run number` -- This determines how many times a model was trained and tested for to take into account of the random seed problem. Each model was trained and tested 8 times thus `run number` ranges from 0-7.\n",
        "3. `Dataset` -- the dataset that the model was trained and tested on. This can only be `Election`, `Restaurant`, or `Laptop`.\n",
        "4. `Data Split` -- which part of the dataset the results are associated with. This can only be `Test` or `Validation`. \n",
        "5. `prediction key` -- This is a unique identifier per model where one trained model can be uniquely identified by combining the `prediction key` and the `run number` columns.\n",
        "\n",
        "Now that we know the data better we can generate the results for both the *Test* and *Validation* data splits. The results generated will be for both *Accuracy* and *Macro F1* metrics, and across all 3 datasets. The results will compare the *CNN (single)* to *CNN (average)*. Lastly as we have run each version of the model **8** times to take into account the random seed problem the results will show the mean score and the standard deviation in brackets.\n",
        "\n",
        "Validation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tocnvZa-wUXM",
        "colab_type": "code",
        "outputId": "73c0af9f-3943-491d-a269-9409449feedd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "val_metric_results = pd.pivot_table(val_result_df, index='data-trained-on', \n",
        "                                    values=['Accuracy', 'Macro F1'], \n",
        "                                    columns='Dataset', \n",
        "                                    aggfunc={'Accuracy': mean_std, \n",
        "                                             'Macro F1': mean_std})\n",
        "val_metric_results"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th colspan=\"3\" halign=\"left\">Accuracy</th>\n",
              "      <th colspan=\"3\" halign=\"left\">Macro F1</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dataset</th>\n",
              "      <th>Election</th>\n",
              "      <th>Laptop</th>\n",
              "      <th>Restaurant</th>\n",
              "      <th>Election</th>\n",
              "      <th>Laptop</th>\n",
              "      <th>Restaurant</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>data-trained-on</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>average</th>\n",
              "      <td>54.07 (0.56)</td>\n",
              "      <td>70.65 (0.68)</td>\n",
              "      <td>72.31 (0.69)</td>\n",
              "      <td>42.74 (2.09)</td>\n",
              "      <td>66.32 (0.96)</td>\n",
              "      <td>60.51 (1.20)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>single</th>\n",
              "      <td>54.54 (0.43)</td>\n",
              "      <td>69.46 (0.72)</td>\n",
              "      <td>71.98 (0.41)</td>\n",
              "      <td>39.62 (1.75)</td>\n",
              "      <td>63.33 (1.70)</td>\n",
              "      <td>58.74 (1.44)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     Accuracy                ...      Macro F1              \n",
              "Dataset              Election        Laptop  ...        Laptop    Restaurant\n",
              "data-trained-on                              ...                            \n",
              "average          54.07 (0.56)  70.65 (0.68)  ...  66.32 (0.96)  60.51 (1.20)\n",
              "single           54.54 (0.43)  69.46 (0.72)  ...  63.33 (1.70)  58.74 (1.44)\n",
              "\n",
              "[2 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsgn9VZ1XB1O",
        "colab_type": "text"
      },
      "source": [
        "Test:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjkvcC2owpgM",
        "colab_type": "code",
        "outputId": "7d754179-52db-4b4c-9bad-d1d156d8134a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "test_metric_results = pd.pivot_table(test_result_df, index='data-trained-on', \n",
        "                                     values=['Accuracy', 'Macro F1'], \n",
        "                                     columns='Dataset', \n",
        "                                     aggfunc={'Accuracy': mean_std, \n",
        "                                              'Macro F1': mean_std})\n",
        "test_metric_results"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th colspan=\"3\" halign=\"left\">Accuracy</th>\n",
              "      <th colspan=\"3\" halign=\"left\">Macro F1</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dataset</th>\n",
              "      <th>Election</th>\n",
              "      <th>Laptop</th>\n",
              "      <th>Restaurant</th>\n",
              "      <th>Election</th>\n",
              "      <th>Laptop</th>\n",
              "      <th>Restaurant</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>data-trained-on</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>average</th>\n",
              "      <td>52.35 (0.69)</td>\n",
              "      <td>68.26 (0.69)</td>\n",
              "      <td>75.81 (0.55)</td>\n",
              "      <td>39.98 (2.20)</td>\n",
              "      <td>60.43 (1.36)</td>\n",
              "      <td>59.40 (1.52)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>single</th>\n",
              "      <td>54.29 (0.73)</td>\n",
              "      <td>65.99 (0.80)</td>\n",
              "      <td>75.19 (0.94)</td>\n",
              "      <td>39.73 (1.88)</td>\n",
              "      <td>55.36 (2.00)</td>\n",
              "      <td>56.71 (1.63)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     Accuracy                ...      Macro F1              \n",
              "Dataset              Election        Laptop  ...        Laptop    Restaurant\n",
              "data-trained-on                              ...                            \n",
              "average          52.35 (0.69)  68.26 (0.69)  ...  60.43 (1.36)  59.40 (1.52)\n",
              "single           54.29 (0.73)  65.99 (0.80)  ...  55.36 (2.00)  56.71 (1.63)\n",
              "\n",
              "[2 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qznzo7twXZra",
        "colab_type": "text"
      },
      "source": [
        "We can see from the results are consistent across the data splits (Test and Validation) as both contain the same ordering where *CNN (avg)* is better than *CNN (single)* for all but the Accuracy on the Election dataset. This therefore shows that in general it is better to use more of the data even if the sentiment label is noisy. \n",
        "\n",
        "However the difference between *CNN (single)* and *CNN (average)* can be quite small, which is shown better below where the table shows the difference between *CNN (avg)* and *CNN (single)*.\n",
        "\n",
        "Validation score differences:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOL4Zw3oF3N6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "5e9e10cc-854b-44ac-c732-ffe2bab32f08"
      },
      "source": [
        "def metric_differences(data_split_df: pd.DataFrame, metric_names: List[str]) -> pd.DataFrame:\n",
        "  '''\n",
        "  Returns the difference between the Average and Single version\n",
        "\n",
        "  :param data_split_df: Dataframe the contains the following columns \n",
        "                        `data-trained-on` and all the strings within the \n",
        "                        `metric_names` argument.\n",
        "  :param metric_names: Names of columns that contain metric scores that are \n",
        "                       to be compared.\n",
        "  :returns: A dataframe that contains new `Difference` columns.\n",
        "  '''\n",
        "  temp_df = data_split_df.copy(deep=True)\n",
        "  temp_df = temp_df.set_index(['Dataset', 'run number'])\n",
        "  average_df = temp_df[temp_df['data-trained-on']=='average']\n",
        "  single_df = temp_df[temp_df['data-trained-on']=='single']\n",
        "  for metric_name in metric_names:\n",
        "    diff_df = average_df[metric_name] - single_df[metric_name]\n",
        "    temp_df[f'Difference {metric_name}'] = diff_df\n",
        "  temp_df = temp_df.reset_index()\n",
        "  temp_df = temp_df[temp_df['data-trained-on']=='average']\n",
        "  return temp_df\n",
        "\n",
        "validation_diff_df = metric_differences(val_result_df, ['Accuracy', 'Macro F1'])\n",
        "pd.pivot_table(validation_diff_df, index='Dataset',\n",
        "               values=['Difference Accuracy', 'Difference Macro F1'], \n",
        "               aggfunc={'Difference Accuracy': mean_std, \n",
        "                        'Difference Macro F1': mean_std})"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Difference Accuracy</th>\n",
              "      <th>Difference Macro F1</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dataset</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Election</th>\n",
              "      <td>-0.48 (0.67)</td>\n",
              "      <td>3.12 (2.58)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Laptop</th>\n",
              "      <td>1.19 (1.14)</td>\n",
              "      <td>2.99 (2.18)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Restaurant</th>\n",
              "      <td>0.34 (0.74)</td>\n",
              "      <td>1.77 (1.69)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           Difference Accuracy Difference Macro F1\n",
              "Dataset                                           \n",
              "Election          -0.48 (0.67)         3.12 (2.58)\n",
              "Laptop             1.19 (1.14)         2.99 (2.18)\n",
              "Restaurant         0.34 (0.74)         1.77 (1.69)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkefO8H5MPT4",
        "colab_type": "text"
      },
      "source": [
        "Test:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I99RWdajY1qY",
        "colab_type": "code",
        "outputId": "284da613-09c9-4acf-bdca-55f27a6fcf41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "test_diff_df = metric_differences(test_result_df, ['Accuracy', 'Macro F1'])\n",
        "pd.pivot_table(test_diff_df, index='Dataset',\n",
        "               values=['Difference Accuracy', 'Difference Macro F1'], \n",
        "               aggfunc={'Difference Accuracy': mean_std, \n",
        "                        'Difference Macro F1': mean_std})"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Difference Accuracy</th>\n",
              "      <th>Difference Macro F1</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Dataset</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Election</th>\n",
              "      <td>-1.94 (1.36)</td>\n",
              "      <td>0.24 (3.00)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Laptop</th>\n",
              "      <td>2.27 (1.00)</td>\n",
              "      <td>5.07 (2.76)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Restaurant</th>\n",
              "      <td>0.62 (1.31)</td>\n",
              "      <td>2.69 (2.82)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           Difference Accuracy Difference Macro F1\n",
              "Dataset                                           \n",
              "Election          -1.94 (1.36)         0.24 (3.00)\n",
              "Laptop             2.27 (1.00)         5.07 (2.76)\n",
              "Restaurant         0.62 (1.31)         2.69 (2.82)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Di31XcF5M7m9",
        "colab_type": "text"
      },
      "source": [
        "As we can see the differences between *CNN (avg)* and *CNN(single)* is very small espically when you take into account the standard deviation. The dataset that the *CNN (avg)* espically performs better is the Laptop dataset. This is most likely due to the fact that the Laptop dataset contains a lot more sentences that only have one unique sentiment (*DS1*).\n",
        "\n",
        "Below we show if the *CNN (avg)* is statistically significantly better then *CNN (single)* using a one-tailed t-test for each of the metrics. For the accuracy metric we use the Welch's t-test as we can assume normality but the macro f1 we cannot therefore we use the Wilcoxon signed-rank test [Dror and Reichart 2018](https://arxiv.org/pdf/1809.01448.pdf). Furthermore we want to know if *CNN (avg)* is statistically significantly better then *CNN (single)* only on the datasets that *CNN (avg)* is better which is all the datasets for the macro-f1 metric and all but the Election for the accuracy metric.\n",
        "\n",
        "Validation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Us41V7H05kxx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def metric_p_values(data_split_df: pd.DataFrame, better_split: str, \n",
        "                    compare_split: str, datasets: List[str], \n",
        "                    metric_names: List[str], assume_metric_normal: List[bool]) -> pd.DataFrame:\n",
        "  '''\n",
        "  Returns the difference between the Average and Single version\n",
        "\n",
        "  :param data_split_df: Dataframe the contains the following columns \n",
        "                        `data-trained-on` and all the strings within the \n",
        "                        `metric_names` argument.\n",
        "  :param metric_names: Names of columns that contain metric scores that are \n",
        "                       to be compared.\n",
        "  :returns: A dataframe that contains new `Difference` columns.\n",
        "  '''\n",
        "  temp_df = data_split_df.copy(deep=True)\n",
        "  better_df = temp_df[temp_df['data-trained-on']==f'{better_split}']\n",
        "  compare_df = temp_df[temp_df['data-trained-on']==f'{compare_split}']\n",
        "  metric_dataset_p_value = defaultdict(dict)\n",
        "  for dataset in datasets:\n",
        "    better_dataset_df = better_df[better_df['Dataset']==dataset]\n",
        "    compare_dataset_df = compare_df[compare_df['Dataset']==dataset]\n",
        "    for metric_index, metric_name in enumerate(metric_names):\n",
        "      assume_normal = assume_metric_normal[metric_index]\n",
        "      better_scores = better_dataset_df[f'{metric_name}']\n",
        "      compare_scores = compare_dataset_df[f'{metric_name}']\n",
        "      p_value = one_tailed_p_value(better_scores, compare_scores, \n",
        "                                   assume_normal=assume_normal)\n",
        "      metric_dataset_p_value[dataset][metric_name] = p_value\n",
        "  return metric_dataset_p_value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-v24okXwwfK9",
        "colab_type": "code",
        "outputId": "5ec813c0-7faf-4149-e8b3-e8be1322fdb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "metric_names = ['Accuracy', 'Macro F1']\n",
        "all_dataset_names = ['Election', 'Laptop', 'Restaurant']\n",
        "\n",
        "validation_p_values = pd.DataFrame(metric_p_values(val_result_df, 'average', \n",
        "                                   'single', all_dataset_names, metric_names, \n",
        "                                   [True, False]))\n",
        "validation_p_values"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/stats/morestats.py:2863: UserWarning: Sample size too small for normal approximation.\n",
            "  warnings.warn(\"Sample size too small for normal approximation.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Election</th>\n",
              "      <th>Laptop</th>\n",
              "      <th>Restaurant</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Accuracy</th>\n",
              "      <td>0.950955</td>\n",
              "      <td>0.003293</td>\n",
              "      <td>0.144942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Macro F1</th>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.017846</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Election    Laptop  Restaurant\n",
              "Accuracy  0.950955  0.003293    0.144942\n",
              "Macro F1  0.005859  0.005859    0.017846"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lv1cUevSSVUy",
        "colab_type": "text"
      },
      "source": [
        "Test:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jejJguTJSWeP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "a49bbf68-6f38-4f1a-cb9e-6ca7faa77927"
      },
      "source": [
        "test_p_values = pd.DataFrame(metric_p_values(test_result_df, 'average', \n",
        "                             'single', all_dataset_names, metric_names, \n",
        "                             [True, False]))\n",
        "test_p_values"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/stats/morestats.py:2863: UserWarning: Sample size too small for normal approximation.\n",
            "  warnings.warn(\"Sample size too small for normal approximation.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Election</th>\n",
              "      <th>Laptop</th>\n",
              "      <th>Restaurant</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Accuracy</th>\n",
              "      <td>0.999921</td>\n",
              "      <td>0.000029</td>\n",
              "      <td>0.077711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Macro F1</th>\n",
              "      <td>0.444319</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.024975</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Election    Laptop  Restaurant\n",
              "Accuracy  0.999921  0.000029    0.077711\n",
              "Macro F1  0.444319  0.005859    0.024975"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtRKuC3iSkgJ",
        "colab_type": "text"
      },
      "source": [
        "We can see above that *CNN (avg)* is only statistically significantly better than *CNN (single)* on the Laptop dataset for both metrics and the Restaurant datsset for for the Macro F1 metric with an $\\alpha < 0.05$. Furthermore if we use the Bonferroni correction to take into account comparing multiple P-Values for each of the metrics we find the following: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-DFSIPwTqyi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "8fa191ca-bc69-4a99-8da7-d3c49c319903"
      },
      "source": [
        "from target_extraction.analysis.statistical_analysis import find_k_estimator\n",
        "\n",
        "print('Validation Split')\n",
        "for metric_name in metric_names:\n",
        "  sig_datasets = find_k_estimator(validation_p_values.loc[f'{metric_name}', :].values, \n",
        "                                  alpha=0.05, method='B')\n",
        "  print(f'For metric {metric_name} there are {sig_datasets} where CNN (avg) is '\n",
        "        'statistically significantly better than CNN (single) with a '\n",
        "        'confindence >=95%')\n",
        "  \n",
        "\n",
        "print('Test Split')\n",
        "for metric_name in metric_names:\n",
        "  sig_datasets = find_k_estimator(test_p_values.loc[f'{metric_name}', :].values, \n",
        "                                  alpha=0.05, method='B')\n",
        "  print(f'For metric {metric_name} there are {sig_datasets} where CNN (avg) is '\n",
        "        'statistically significantly better than CNN (single) with a '\n",
        "        'confindence >=95%')"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation Split\n",
            "For metric Accuracy there are 1 where CNN (avg) is statistically significantly better than CNN (single) with a confindence >=95%\n",
            "For metric Macro F1 there are 3 where CNN (avg) is statistically significantly better than CNN (single) with a confindence >=95%\n",
            "Test Split\n",
            "For metric Accuracy there are 1 where CNN (avg) is statistically significantly better than CNN (single) with a confindence >=95%\n",
            "For metric Macro F1 there are 2 where CNN (avg) is statistically significantly better than CNN (single) with a confindence >=95%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osENbHXhVieZ",
        "colab_type": "text"
      },
      "source": [
        "To conclude it is better to use the *CNN (avg)* as a strong baseline across the datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkjPJ7dqVShh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}